version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-system-api
    restart: unless-stopped
    environment:
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL_NAME=${OPENROUTER_MODEL_NAME:-meta-llama/llama-3.3-70b-instruct}
      - TAVILY_API_KEY=${TAVILY_API_KEY:-}
      - HF_TOKEN=${HF_TOKEN}
      - USE_HF_EMBEDDINGS=${USE_HF_EMBEDDINGS:-true}
      - HF_LOGGING=${HF_LOGGING:-true}
      - HF_API_URL=${HF_API_URL:-https://api-inference.huggingface.co/models/BAAI/bge-small-en-v1.5}
      - USER_AGENT=${USER_AGENT:-RAG-System/1.0}
    volumes:
      - ./data:/app/data:ro
      - ./vector_store:/app/vector_store
      - ./scenarios:/app/scenarios:ro
    networks:
      - rag-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  caddy:
    image: caddy:2-alpine
    container_name: rag-system-caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - rag-network
    depends_on:
      - api

networks:
  rag-network:
    driver: bridge

volumes:
  caddy_data:
  caddy_config:

